{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from hmmlearn import hmm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File D:\\EOD\\EOD_20210908.csv does not exist: 'D:\\\\EOD\\\\EOD_20210908.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-19052be5cdb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#~/Desktop/Corbin SBU/AMS 520/Project/BofA Projects Data/EOD_20210908.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m input_df = pd.read_csv('D:\\\\EOD\\\\EOD_20210908.csv',\n\u001b[0m\u001b[1;32m      3\u001b[0m                        \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        names = ['Ticker', # Label columns\n\u001b[1;32m      5\u001b[0m                                 \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File D:\\EOD\\EOD_20210908.csv does not exist: 'D:\\\\EOD\\\\EOD_20210908.csv'"
     ]
    }
   ],
   "source": [
    "# Input data (two first lines for Corbin and Young)\n",
    "\n",
    "# input_df = pd.read_csv('D:\\\\EOD\\\\EOD_20210908.csv',\n",
    "input_df = pd.read_csv('~/Desktop/Corbin SBU/AMS 520/Project/BofA Projects Data/EOD_20210908.csv',\n",
    "                       header = None,\n",
    "                       names = ['Ticker', # Label columns\n",
    "                                'Date',\n",
    "                                'Open',\n",
    "                                'High',\n",
    "                                'Low',\n",
    "                                'Close',\n",
    "                                'Volume',\n",
    "                                'Dividend',\n",
    "                                'Stock_split',\n",
    "                                'Adj_open',\n",
    "                                'Adj_high',\n",
    "                                'Adj_low',\n",
    "                                'Adj_close',\n",
    "                                'Adj_volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input_df.loc[input_df['Ticker'] == 'SPY'] # Select which index to use for analysis\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Proposed Idea: Create a HMM for the recent Neff days, and for all days after the Neff'th day\n",
    "# Predict which state we are currently in based on the Neff recent days\n",
    "# Be fully invested if in a positive market, fully divested in a negative market\n",
    "\n",
    "Neff = 260 #Length of Lookback\n",
    "Return = 100*(data['Adj_close'] - data['Adj_close'].shift(1)) / data['Adj_close'].shift(1) #return daily percentage returns.\n",
    "data['Return'] = Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookback = neff\n",
    "def get_hvol_yz(df, lookback=Neff):\n",
    "    \"\"\"\n",
    "    Funtion create a serie of OHLC volatility using length of Lookback and data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : data frame that has Adj_open, Adj_high, Adj_low, Adj_close\n",
    "    lookback : length of lookback\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    percentage of volatility at each date\n",
    "    \"\"\"\n",
    "    o = df.Adj_open\n",
    "    h = df.Adj_high\n",
    "    l = df.Adj_low\n",
    "    c = df.Adj_close\n",
    "    \n",
    "    k = 0.34 / (1.34 + (lookback+1)/(lookback-1))\n",
    "    cc = np.log(c/c.shift(1))\n",
    "    ho = np.log(h/o)\n",
    "    lo = np.log(l/o)\n",
    "    co = np.log(c/o)\n",
    "    oc = np.log(o/c.shift(1))\n",
    "    oc_sq = oc**2\n",
    "    cc_sq = cc**2\n",
    "    rs = ho*(ho-co)+lo*(lo-co)\n",
    "    #close_vol = pd.rolling_sum(cc_sq, window=lookback) * (1.0 / (lookback - 1.0))\n",
    "    close_vol = cc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "\n",
    "    #open_vol = pd.rolling_sum(oc_sq, window=lookback) * (1.0 / (lookback - 1.0))\n",
    "    open_vol = oc_sq.rolling(lookback).sum()  * (1.0 / (lookback - 1.0))\n",
    "    \n",
    "    #window_rs = pd.rolling_sum(rs, window=lookback) * (1.0 / (lookback - 1.0))\n",
    "    window_rs = rs.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    \n",
    "    result = (open_vol + k * close_vol + (1-k) * window_rs).apply(np.sqrt) * np.sqrt(252)\n",
    "    result[:lookback-1] = 0.0\n",
    "    \n",
    "    return  result * 100\n",
    "\n",
    "new_vol=get_hvol_yz(data,lookback = 260)\n",
    "data['Volatility'] = new_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources: stlouisfed.org, yahoo finance\n",
    "# Helpful guids: https://rdrr.io/github/AndreMikulec/econModel/src/R/StressIndex.R\n",
    "#~/Desktop/Corbin SBU/AMS 520/Project/Project Code/GitHub/Regime-Detection-HMM/Regression_Variables.xlsx\n",
    "# Regression to use to predict index\n",
    "reg_data = pd.read_excel('C:\\\\Users\\\\ryans\\\\Desktop\\\\AMS\\\\520\\\\Regression_Variables.xlsx')\n",
    "\n",
    "reg_data.rename(columns={'observation_date': 'Date'}, inplace=True)\n",
    "reg_data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = data.join(reg_data, how='outer').loc[data.index[0]:data.index[-1],].dropna(subset=['Ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=['Ticker',\n",
    "                          'Open',\n",
    "                          'High',\n",
    "                          'Low',\n",
    "                          'Close',\n",
    "                          'Volume',\n",
    "                          'Dividend',\n",
    "                          'Stock_split',\n",
    "                          'Adj_open',\n",
    "                          'Adj_high',\n",
    "                          'Adj_low',\n",
    "                          'Adj_close',\n",
    "                          'Adj_volume',\n",
    "                          'Volatility'],\n",
    "                inplace=True)\n",
    "\n",
    "# Fill all missing data points with the most recently available value\n",
    "merged_data.fillna(method = 'ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regression_pred = np.array([])\n",
    "for i in range(len(data) - 2*Neff):\n",
    "\n",
    "    #print(i)\n",
    "    \n",
    "    na_check = pd.isna(merged_data.iloc[0 + i:Neff + i,1:])\n",
    "    ind_var_available = np.array([])\n",
    "    \n",
    "    for j, k in enumerate(na_check):\n",
    "        if not any(na_check.loc[:,k]):\n",
    "            ind_var_available = np.append(ind_var_available, j+1)\n",
    "            \n",
    "    ind_var_available = ind_var_available.astype(np.int)\n",
    "    \n",
    "    X = merged_data.iloc[0+i:Neff+i,ind_var_available].to_numpy()\n",
    "    y = merged_data['Return'][i+1:Neff+1+i].to_numpy()\n",
    "    \n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X,y)\n",
    "    regression_pred = np.append(regression_pred, lin_reg.predict([X[-1]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cv = 10\n",
    "regression_pred = np.array([])\n",
    "\n",
    "for i in range(len(data) - 2*Neff):\n",
    "\n",
    "    #print(i)\n",
    "    \n",
    "    na_check = pd.isna(merged_data.iloc[0 + i:Neff + i,1:])\n",
    "    ind_var_available = np.array([])\n",
    "    \n",
    "    for j, k in enumerate(na_check):\n",
    "        if not any(na_check.loc[:,k]):\n",
    "            ind_var_available = np.append(ind_var_available, j+1)\n",
    "            \n",
    "    ind_var_available = ind_var_available.astype(np.int)\n",
    "\n",
    "    X = merged_data.iloc[0+i:Neff+i,ind_var_available].to_numpy()\n",
    "    y = merged_data['Return'][i+1:Neff+1+i].to_numpy()    \n",
    "\n",
    "    lassoCV = LassoCV(cv=cv)\n",
    "    lassoCV.fit(X, y)\n",
    "#     print(len(lassoCV.coef_))\n",
    "#     print(lassoCV.predict([X[-1]]))\n",
    "#     print(y[-1])\n",
    "    regression_pred = np.append(regression_pred, lassoCV.predict([X[-1]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_predicted_returns = pd.DataFrame(index=data.index[-len(regression_pred):],\n",
    "                                     data=regression_pred,\n",
    "                                     columns=['Regression_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_predicted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge reg_predicted_returns to data\n",
    "data = data.join(reg_predicted_returns, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a HMM\n",
    "states = 2\n",
    "max_iterations = 100 # For EM algorithm\n",
    "\n",
    "current_state = np.array([]) # Initialize an array to track the current regimes\n",
    "\n",
    "# Exclude first (to calculate trailing volatility) and\n",
    "# second (to have a full set of observations to fit a HMM) Neff observations\n",
    "for i in range(Neff+1, len(data) - 2*Neff):\n",
    "\n",
    "    # Initialize a Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components = states, covariance_type=\"full\", n_iter = max_iterations);\n",
    "    \"\"\"\n",
    "    Representation of a hidden Markov model probability distribution. \n",
    "    This class allows for easy evaluation of, sampling from, and maximum-likelihood estimation of the parameters of a HMM\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components: Number of States in the model\n",
    "    covariance_type: String describing the type of covariance parameters to use.\n",
    "                     Must be one of ‘spherical’, ‘tied’, ‘diag’, ‘full’. Defaults to ‘diag’.\n",
    "    n_iter : Number of iterations to perform.\n",
    "    \"\"\"\n",
    "    # Pull Neff observations of Volatility and Returns\n",
    "    observations = data.iloc[Neff+i:Neff*2+i,:].loc[:,['Volatility', 'Return', 'Regression_prediction']].to_numpy()\n",
    "\n",
    "    model.fit(observations) # Fit the model to the observations\n",
    "    \"\"\"\n",
    "    Estimate model parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    observations : List of array-like observation sequences (shape (n_i, n_features)).\n",
    "    \"\"\"\n",
    "    #print(f'i = {i}') # Print to ensure loop is running\n",
    "\n",
    "    # Model randomly allocates a '0' or a '1' to a state, so check which state has the higher mean return\n",
    "    if model.means_[0,1] > model.means_[1,1]:\n",
    "        positive_state = 0\n",
    "    else:\n",
    "        positive_state = 1\n",
    "\n",
    "    predictions = model.predict(observations) # Predict the state for each observation\n",
    "    \"\"\"\n",
    "    Find most likely state sequence corresponding to obs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observations : List of n_features-dimensional data points. Each row corresponds to a single data point.\n",
    "    \"\"\"\n",
    "    if positive_state == 1: # If the state with the higer mean return is state 1, do nothing, if not...\n",
    "        pass\n",
    "    \n",
    "    else: # Switch the predicted regimes to ensure state 1 is always the regime with a greater mean return\n",
    "        zeros = np.where(predictions == 0)\n",
    "        ones = np.where(predictions == 1)\n",
    "        predictions[zeros] = 1\n",
    "        predictions[ones] = 0\n",
    "        \n",
    "    # Append the current state to the regime tracker\n",
    "    current_state = np.append(current_state,predictions[-1])\n",
    "        \n",
    "# Switch values from type float to type int for later calculations\n",
    "current_state = current_state.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of regime change in the data sets\n",
    "total_regime_switches = sum(np.abs(current_state[1:] - current_state[:-1]))\n",
    "# how many years exist in the data\n",
    "years_of_data = len(current_state) / 252 # Approximately 252 trading days in a year\n",
    "avg_regime_changes_per_year = total_regime_switches / years_of_data\n",
    "\n",
    "print(f'Total regime switches: {total_regime_switches}')\n",
    "print(f'Average number of regime switches per year: {avg_regime_changes_per_year:.01f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regime switches to graphically analyze\n",
    "\n",
    "dates = [datetime.datetime.strptime(d,\"%Y-%m-%d\").date() for d in data.index[3*Neff + 1:]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,3))\n",
    "\n",
    "formatter = mdates.DateFormatter(\"%Y\")\n",
    "\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "fmt_half_year = mdates.MonthLocator(interval=24)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "ax.plot(dates, current_state, '.', color = 'royalblue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_0 = np.where(current_state == 0)[0] + 3*Neff + 1\n",
    "state_1 = np.where(current_state == 1)[0] + 3*Neff + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and view statistics of the two states\n",
    "\n",
    "print(f'Number of occurences of state 0: {len(state_0)}')\n",
    "print(f'Mean return of state 0: {data.iloc[state_0].Return.mean():.03f}')\n",
    "print(f'Volatility of state 0: {data.iloc[state_0].Return.std():.03f}')\n",
    "print('\\n')\n",
    "print(f'Number of occurences of state 1: {len(state_1)}')\n",
    "print(f'Mean return of state 1: {data.iloc[state_1].Return.mean():.03f}')\n",
    "print(f'Volatility of state 1: {data.iloc[state_1].Return.std():.03f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the returns of the data, segmented by hidden state\n",
    "\n",
    "plot = sns.relplot(x = range(0,len(current_state)),\n",
    "                   y = \"Adj_close\",\n",
    "                   data = data[3*Neff+1:],\n",
    "                   hue = current_state,\n",
    "                   linewidth = 0,\n",
    "                   palette = \"Set2\",\n",
    "                   s = 10);\n",
    "\n",
    "plot.fig.set_size_inches(18,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invest in treasury if divested\n",
    "\n",
    "daily_10_year_treas = reg_data.loc[:,'Var3_10_Yr_Treas']**(1/252) - 1\n",
    "daily_10_year_treas.fillna(method = 'ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_10_year_treas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(daily_10_year_treas, how='outer').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns = current_state * data.Return[Neff+1:] / 100 + (1 - current_state) * data.Var3_10_Yr_Treas[Neff+1:] / 100 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the growth of a theoretical portfolio\n",
    "# Assume fully invested if state 1, fully divested if state 0\n",
    "\n",
    "current_portfolio = data.Adj_close[Neff+1] * np.cumprod(portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the return portfolio dynamics\n",
    "\n",
    "plot = sns.relplot(x = range(0,len(current_state)),\n",
    "                   y = \"Adj_close\",\n",
    "                   data = data[Neff+1:],\n",
    "                   hue = current_state,\n",
    "                   linewidth = 0,\n",
    "                   palette = \"Set2\",\n",
    "                   s = 10);\n",
    "\n",
    "plt.plot(range(0,len(current_state)), current_portfolio, color='royalblue')\n",
    "plt.legend(['HMM Portfolio','SPY (state 1)', 'SPY (state 0)'])\n",
    "\n",
    "plot.fig.set_size_inches(14,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_portfolio_returns = current_portfolio.pct_change(1)\n",
    "current_portfolio_sharpe = current_portfolio_returns.mean()/current_portfolio_returns.std()\n",
    "current_portfolio_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('test_portfolio_500.csv', current_portfolio, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import previous iterations using different Neff window lengths\n",
    "test_portfolio_750 = np.loadtxt(\"test_portfolio_750.csv\")\n",
    "test_portfolio_500 = np.loadtxt(\"test_portfolio_500.csv\")\n",
    "test_portfolio_250 = np.loadtxt(\"test_portfolio_250.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,4))\n",
    "\n",
    "ax.plot(range(0,len(current_state)+500), data.Adj_close[2*250+1:], color='slategray');\n",
    "ax.plot(range(1000,len(current_state)+500), test_portfolio_750, color='royalblue');\n",
    "ax.plot(range(500,len(current_state)+500), test_portfolio_500, color='darkgreen');\n",
    "ax.plot(range(0,len(current_state)+500), test_portfolio_250, color='darkviolet');\n",
    "\n",
    "ax.legend(['SPY','HMM (750-day rolling window)', 'HMM (500-day rolling window)', 'HMM (250-day rolling window)']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Empirical results\n",
    "\n",
    "spy_mean_return = data.Return[2*Neff+1:].mean()\n",
    "spy_vol = data.Return[2*Neff+1:].std()\n",
    "spy_sharpe = data.Return[2*Neff+1:].mean() / data.Return[2*Neff+1:].std()\n",
    "\n",
    "test_portfolio_750_returns = (test_portfolio_750[1:] - test_portfolio_750[:-1]) / test_portfolio_750[:-1]\n",
    "test_portfolio_750_sharpe = test_portfolio_750_returns.mean()/test_portfolio_750_returns.std()\n",
    "\n",
    "test_portfolio_500_returns = (test_portfolio_500[1:] - test_portfolio_500[:-1]) / test_portfolio_500[:-1]\n",
    "test_portfolio_500_sharpe = test_portfolio_500_returns.mean()/test_portfolio_500_returns.std()\n",
    "\n",
    "test_portfolio_250_returns = (test_portfolio_250[1:] - test_portfolio_250[:-1]) / test_portfolio_250[:-1]\n",
    "test_portfolio_250_sharpe = test_portfolio_250_returns.mean()/test_portfolio_250_returns.std()\n",
    "\n",
    "print(f'SPY average return: {spy_mean_return:.03f}')\n",
    "print(f'SPY volatility: {spy_vol:.03f}')\n",
    "print(f'SPY Sharpe Ratio: {spy_sharpe:.03f}')\n",
    "print('\\n')\n",
    "print(f'HMM (750-day rolling window) average return: {data.Return[2*Neff+1:].mean() :.05f}')\n",
    "print(f'HMM (750-day rolling window) volatility: {data.Return[2*Neff+1:].std():.05f}')\n",
    "print(f'HMM (750-day rolling window) Sharpe Ratio: {test_portfolio_750_sharpe:.03f}')\n",
    "print('\\n')\n",
    "print(f'HMM (500-day rolling window) average return: {test_portfolio_500_returns.mean():.05f}')\n",
    "print(f'HMM (500-day rolling window) volatility: {test_portfolio_500_returns.std():.05f}')\n",
    "print(f'HMM (500-day rolling window) Sharpe Ratio: {test_portfolio_500_sharpe:.03f}')\n",
    "print('\\n')\n",
    "print(f'HMM (250-day rolling window) average return: {test_portfolio_250_returns.mean():.05f}')\n",
    "print(f'HMM (250-day rolling window) volatility: {test_portfolio_250_returns.std():.05f}')\n",
    "print(f'HMM (250-day rolling window) Sharpe Ratio: {test_portfolio_250_sharpe:.03f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
